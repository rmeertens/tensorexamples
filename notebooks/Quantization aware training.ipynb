{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization aware training for tensorflow-lite\n",
    "If you want to run your TensorFlow code on an embedded platform you want to quantize your neural network. Especially edge-tpu devices or raspberry pi devices are very suitable for running quantized code. However, when quantizing your neural network it's possible that the performance goes down. What helps against this is running quantization aware training, where your model already experiences a downgrade in performance and is able to adjust to it. \n",
    "\n",
    "In case you would like to know why and how this works, take a look at these articles: https://blog.tensorflow.org/2020/04/quantization-aware-training-with-tensorflow-model-optimization-toolkit.html, https://www.tensorflow.org/model_optimization/guide/quantization/training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: using the tensorflow-model-optimization library\n",
    "I think the easiest option is to use the tensorflow-model-optimization library. If you install this library you literally only have to add one line of code to quantize your Keras model during training. Below I created a very simple MNIST example. I personally think it's best to start training in a quantization aware way immediately. However, it's also possible to add the quantization layers at a later stage so you have both a normal model (maybe for a big server) and a quantization aware model (maybe for a raspberry pi or a robot). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: tensorflow-model-optimization in /root/.local/lib/python3.6/site-packages (0.3.0)\n",
      "Requirement already satisfied, skipping upgrade: dm-tree~=0.1.1 in /root/.local/lib/python3.6/site-packages (from tensorflow-model-optimization) (0.1.5)\n",
      "Requirement already satisfied, skipping upgrade: six~=1.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-optimization) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy~=1.14 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-optimization) (1.18.4)\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Method 1: tensorflow-model-optimization library\n",
    "!pip3 install --user --upgrade tensorflow-model-optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The TensorFlow version used in this tutorial is 2.2.0\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "quant_flatten_2 (QuantizeWra (None, 784)               1         \n",
      "_________________________________________________________________\n",
      "quant_dense_4 (QuantizeWrapp (None, 128)               100485    \n",
      "_________________________________________________________________\n",
      "quant_dense_5 (QuantizeWrapp (None, 10)                1295      \n",
      "=================================================================\n",
      "Total params: 101,781\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 11\n",
      "_________________________________________________________________\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2556 - accuracy: 0.9277 - val_loss: 0.1357 - val_accuracy: 0.9600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6fc1fa7048>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "print(\"The TensorFlow version used in this tutorial is\", tf.__version__)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Transform the input into floating point inputs between 0 and 1\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Define a very simple model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(28,28)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128,activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Print the summary of the object before making it quantization aware\n",
    "model.summary()\n",
    "\n",
    "# The one line to add quantization aware layers around the existing layers of your model. \n",
    "model = tfmot.quantization.keras.quantize_model(model)\n",
    "\n",
    "# Print the summary of the object after making it quantization aware\n",
    "model.summary()\n",
    "\n",
    "# Compile the model. You always have to do that after quantizing your weights. \n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Train for one epoch to verify this works. \n",
    "model.fit(x_train, y_train,epochs=1, validation_data = (x_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: using tf.quantization.fake_quant_with_min_max_args\n",
    "\n",
    "If you want a bit more control over your fake quantization you can use the `tf.quantization.fake_quant_with_min_max_args` function. This function immitates the quantization with parameters you can enter yourself. If you would like to have specific ranges, you can specify these already. What's also nice is that you can specify the number of bits with which you are going to perform inference. You might want to do 8-bit inference for things like the NVIDIA Jetson, or even 4-bit inference if you have access to Turing TensorCores. \n",
    "\n",
    "What's also nice is that you can choose what to quantize and what not. Perhaps you discovered that the backbone should be quantized, but that you want to keep your head unquantized. Then you don't have to use quantization aware training... \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "tf_op_layer_FakeQuantWithMin [(None, 784)]             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "tf_op_layer_FakeQuantWithMin [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create a simple quantization aware dense layer\n",
    "def quantization_aware_dense_layer(num_output_neurons, activation, x):\n",
    "    x = tf.quantization.fake_quant_with_min_max_args(x)\n",
    "    x = tf.keras.layers.Dense(num_output_neurons,activation=activation)(x)\n",
    "    return x\n",
    "\n",
    "# Define a very simple model using our new layer\n",
    "input_layer = tf.keras.layers.Input(shape=(28,28))\n",
    "x = tf.keras.layers.Flatten()(input_layer)\n",
    "x = quantization_aware_dense_layer(128, 'relu',x)\n",
    "output_layer = quantization_aware_dense_layer(10, 'softmax',x)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_layer], outputs=[output_layer])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2606 - accuracy: 0.9256 - val_loss: 0.1423 - val_accuracy: 0.9577\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6fd763a518>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model. \n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Train for one epoch to verify this works. \n",
    "model.fit(x_train, y_train,epochs=1, validation_data = (x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing these fake quantization layers again\n",
    "\n",
    "Some tools can't really handle fake quantization layers. Luckily, especially for sequential models, it's easy to remove these layers again. In this case you can simply define a new sequential model, and leave out the fake quantization layers with a simple string filter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_3 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1423 - accuracy: 0.9580\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.14229463040828705, 0.9580000042915344]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model = tf.keras.Sequential([layer for layer in model.layers if 'FakeQuantWithMinMaxArgs' not in layer.name])\n",
    "\n",
    "new_model.summary()\n",
    "\n",
    "# You have to re-compile the model. Note that this does NOT reset the weights!\n",
    "new_model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "new_model.evaluate(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
