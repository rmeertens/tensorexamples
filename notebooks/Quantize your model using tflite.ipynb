{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization in tensorflow-lite\n",
    "If you want to run your TensorFlow code on an embedded platform you want to quantize your neural network. Especially edge-tpu devices or raspberry pi devices are very suitable for running quantized code. \n",
    "\n",
    "In this post I will show you how to go from training a simple neural network to running a quantized version of this network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: train a model \n",
    "For the sake of having a model to quantize we are building a simple classifier for MNIST digits. What model you use exactly doesn't really matter, so I will take an easy one here. Feel free to experiment and make the model better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The TensorFlow version used in this tutorial is 2.2.0\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2602 - accuracy: 0.9253\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0f8a5cfac8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "\n",
    "print(\"The TensorFlow version used in this tutorial is\", tf.__version__)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Transform the input into floating point inputs between 0 and 1\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Define a very simple model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(28,28)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128,activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile and train the model for one epoch... It's only to have something trained, not get the best score\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "model.fit(x_train, y_train,epochs=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: save the model \n",
    "\n",
    "Imagine that you finally trained the perfect image classification algorithm! Naturally you save it to be able to load it later, or evaluate in different environments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: saved_models/saved_quantization_model/assets\n"
     ]
    }
   ],
   "source": [
    "saved_model_dir = 'saved_models/saved_quantization_model'\n",
    "model.save(saved_model_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Convert the model to a tflite model\n",
    "\n",
    "Now is the moment to take your model and turn it into a calibrated model. To calibrate your model you show the neural network multiple possible inputs. In the background the activations are calculated to get a feeling for the spread of the activations. This is why it's important to make the calibration dataset representative for your use case. \n",
    "\n",
    "If you during this step never show an image of a certain number/class it is possible that your model in production has issues recognizing this number/class. For now I just take a low amount of samples to speed up the process. \n",
    "\n",
    "In this tutorial we are making all inference run in int8 values. The range of int8 is very low, there are now only 255 options for each activation in each layer. This will let you lose a lot of precision in your neural network. Always make sure in an evaluation step that your model is still able to detect everything. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_calibration_steps = 150\n",
    "\n",
    "# Load the model we saved in step 2, and set the optimizations\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# Make a way to load a representativce dataset. \n",
    "def representative_dataset_gen():\n",
    "    sample_per_calibration_step = 8\n",
    "    for calib_index in range(num_calibration_steps):\n",
    "        # Get sample input data as a numpy array. You can either randomly select, or have a fixed calibration dataset. \n",
    "        yield [[x_train[sample_per_calibration_step,...]]]\n",
    "    \n",
    "# Choose the inference input and output, and set the supported ops.     \n",
    "converter.representative_dataset = representative_dataset_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8  # or tf.uint8\n",
    "converter.inference_output_type = tf.int8  # or tf.uint8\n",
    "tflite_quant_model = converter.convert()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: save the tflite model\n",
    "So far your work has been on a big computer. Now it's possible to transport your model to a low power device, such as your raspberry pi. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104848"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_quant_model_save_path = 'saved_models/model_quant.tflite'\n",
    "NUM_EVALUATE_SAMPLES = 128\n",
    "\n",
    "open(tflite_quant_model_save_path, \"wb\").write(tflite_quant_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: load the tflite model and run inference. \n",
    "This is all code which is necessary to run inference on your embedded device :) \n",
    "Once you have this all looks really simple, right? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction results shape: (128, 10)\n"
     ]
    }
   ],
   "source": [
    "# Load quantized TFLite model\n",
    "tflite_interpreter_quant = tf.lite.Interpreter(model_path=tflite_quant_model_save_path)\n",
    "\n",
    "# Set the input and output tensors to handle a small batch of images to evaluate on\n",
    "input_details = tflite_interpreter_quant.get_input_details()\n",
    "output_details = tflite_interpreter_quant.get_output_details()\n",
    "tflite_interpreter_quant.resize_tensor_input(input_details[0]['index'], (NUM_EVALUATE_SAMPLES, 28, 28))\n",
    "tflite_interpreter_quant.resize_tensor_input(output_details[0]['index'], (NUM_EVALUATE_SAMPLES, 10))\n",
    "tflite_interpreter_quant.allocate_tensors()\n",
    "\n",
    "# Run inference on the first set of test images\n",
    "tflite_interpreter_quant.set_tensor(input_details[0]['index'], x_test[:NUM_EVALUATE_SAMPLES, ...])\n",
    "tflite_interpreter_quant.invoke()\n",
    "\n",
    "tflite_q_model_predictions = tflite_interpreter_quant.get_tensor(output_details[0]['index'])\n",
    "print(\"\\nPrediction results shape:\", tflite_q_model_predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: evaluate your model performance\n",
    "As I mentioned in a previous step your model performance might have gone down. It is important to evaluate both your general performance as well as your performance for specific classes. Things which are rare or very similar to other objects might not be represented very well anymore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.9609375\n",
      "Confusion matrix\n",
      "[[10  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 15  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  9  1  0  0  0  0  0  0]\n",
      " [ 0  0  0 12  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 20  0  0  0  0  0]\n",
      " [ 0  0  0  1  0  8  1  0  0  0]\n",
      " [ 0  0  0  0  0  0 12  0  0  0]\n",
      " [ 0  0  0  0  1  0  0 18  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  3  0]\n",
      " [ 0  0  0  0  1  0  0  0  0 16]]\n"
     ]
    }
   ],
   "source": [
    "# Get the true values and the predictions for the first N samples\n",
    "y_true = y_test[:NUM_EVALUATE_SAMPLES]\n",
    "y_pred = np.argmax(tflite_q_model_predictions, axis=1)\n",
    "\n",
    "# Calculate the accuracy and confusion matrics with sklearn\n",
    "accuracy_score = sklearn.metrics.accuracy_score(y_true, y_pred)\n",
    "confusion_mat = sklearn.metrics.confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Print the accuracy score and confusion matrix\n",
    "print(\"Accuracy score:\", accuracy_score)\n",
    "print(\"Confusion matrix\")\n",
    "print(confusion_mat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Now you are able to run inference on low-power embedded devices! Enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
