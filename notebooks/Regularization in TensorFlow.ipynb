{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training a neural network it's easy to overfit to your training dataset. One of the ways to prevent that is using so-call regularization techniques. In this post I will go over some common ways to regularize your network. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default network \n",
    "Before we start regularizing I will start with a simple MNIST convolutional network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The TensorFlow version used in this tutorial is 2.2.0\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 16)        4624      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                125450    \n",
      "=================================================================\n",
      "Total params: 130,394\n",
      "Trainable params: 130,394\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1875/1875 [==============================] - 33s 17ms/step - loss: 0.1488 - accuracy: 0.9561\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcd3a73f128>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"The TensorFlow version used in this tutorial is\", tf.__version__)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Transform the input into floating point inputs between 0 and 1\n",
    "x_train = np.expand_dims(x_train.astype('float32') / 255.0, -1)\n",
    "x_test = np.expand_dims(x_test.astype('float32') / 255.0, -1)\n",
    "\n",
    "def get_model():\n",
    "    # Define a very simple model\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Input(shape=(28,28, 1)),\n",
    "        tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same'),\n",
    "        tf.keras.layers.Conv2D(16, 3, activation='relu', padding='same'),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# Compile and train the model for one epoch... It's only to have something trained, not get the best score\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "model.fit(x_train, y_train,epochs=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "One of the simplest ways to regularize your output is by using the so-called dropout technique. With this method random values in the layers of\n",
    " your neural network will be set to zero. The underlying idea is that the neural network can't overfit to one specific feature, but instead has to rely on multiple features to be present to recognize your objects. \n",
    "\n",
    "Adding dropout is simple, it's a layer available in Keras at `tf.keras.layers.Dropout(dropout_percentage)`. The dropout percentage is something you can set yourself. By default it's set to 0.5. \n",
    "Personally I noticed that if you have a hard problem with enough data, and a wide variety of data, dropout should be quite low to be effective. However, if you have a simple problem and not enough data it's better to increase the dropout percentage to make the most of your data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 28, 28, 16)        4624      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 28, 28, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                125450    \n",
      "=================================================================\n",
      "Total params: 130,394\n",
      "Trainable params: 130,394\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1875/1875 [==============================] - 46s 24ms/step - loss: 0.1655 - accuracy: 0.9512\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcd2c50b860>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(28,28, 1)),\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Conv2D(16, 3, activation='relu', padding='same'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# Compile and train the model for one epoch... It's only to have something trained, not get the best score\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "model.fit(x_train, y_train,epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight decay\n",
    "When training neural network it's possible for weights to become very large. It's possible to solve a problem that way, but in general this makes the network more vulnerable for weird inputs, as the large multiplications can then stack up giving bad results. It's better for weights to be small to solve the problem, as this tends to generalize better to the whole problem space. \n",
    "\n",
    "To prevent weights becoming really big you use so-called weight decay, or weight regularization. The idea is that you give a penalty to the neural network for big weights every training step. This way gradient descent will try to balance out performance with the values of the weights. \n",
    "\n",
    "There are two common methods: L1 regularization and L2 regularization. L1 regularization adds a cost proportional to the absolute value of the weights. L2 regularization goes a step further and squares the weights. When you use L1 regularization you punish each weight equally hard when this weight is not zero. With L2 regularization you punish large weights way harder down than small weights. \n",
    "\n",
    "I personally like using L2 loss better, as small weigths will hardly be affected, and large weights are punished hard. \n",
    "\n",
    "It's not hard to add weight decay to your model. TensorFlows `tf.keras.layers.Conv2D` already has a keyword to add a regularization to your layer. You have to specify the balance of your normal loss and weight decay though. Regularizers that are available in `tf.keras.regularizers` are: \n",
    "* L1: If you pass the value of 0.001 you will add `0.001 * abs(weight_value)` to the total loss of your neural network.\n",
    "* L2: If you pass the value of 0.001 you will add `0.001 * weight_value**2` to the total loss of your neural network.\n",
    "* L1L2: If you pass the values of 0.002 and 0.001 you will add `0.002 * abs(weight_value) + 0.001 * weight_value**2` to the total loss of your neural network.\n",
    "\n",
    "Note that for your layers you can add a regularisation for your kernel (your multiplication) and for your bias (the addition after your multiplication). I personally mostly saw a benefit for the multiplication, but it's good to be aware of another variable your can use weight decay on. \n",
    "\n",
    "If you are training a neural network it's not important to not only look at the normal loss, but at the two components of the loss. If you don't it's entirely possible that your original goal is not achieved anymore. \n",
    "\n",
    "Before showing an example of how you can add the regularisation to your neural network it's interesting to plot the weights we got in the first layer of the previous network (with Dropout). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.,  0.,  7., 14., 11., 16., 24., 37., 31., 40., 41., 35., 23.,\n",
       "         5.,  3.]),\n",
       " array([-0.40034324, -0.3540591 , -0.30777496, -0.26149082, -0.21520667,\n",
       "        -0.16892253, -0.12263838, -0.07635424, -0.03007009,  0.01621405,\n",
       "         0.06249819,  0.10878234,  0.15506648,  0.20135061,  0.24763477,\n",
       "         0.2939189 ], dtype=float32),\n",
       " <a list of 15 Patch objects>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(model.layers[0].kernel.numpy().reshape(-1), bins=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 28, 28, 16)        4624      \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                125450    \n",
      "=================================================================\n",
      "Total params: 130,394\n",
      "Trainable params: 130,394\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1875/1875 [==============================] - 34s 18ms/step - loss: 0.2390 - accuracy: 0.9495\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcd2c08e978>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(28,28, 1)),\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Conv2D(16, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# Compile and train the model for one epoch... It's only to have something trained, not get the best score\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "model.fit(x_train, y_train,epochs=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the weights of the first layer again. As you can see they are a bit smaller due to the regularization we applied on them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([10.,  7., 11., 25., 29., 69., 30., 26., 18., 27., 13., 15.,  4.,\n",
       "         1.,  3.]),\n",
       " array([-0.12597261, -0.10098308, -0.07599354, -0.051004  , -0.02601446,\n",
       "        -0.00102493,  0.02396461,  0.04895415,  0.07394368,  0.09893322,\n",
       "         0.12392276,  0.1489123 ,  0.17390184,  0.19889137,  0.22388092,\n",
       "         0.24887045], dtype=float32),\n",
       " <a list of 15 Patch objects>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(model.layers[0].kernel.numpy().reshape(-1), bins=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing which is now present are the losses in the model. You can also see how big they are: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=0.018498313>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.023712082>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.losses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
